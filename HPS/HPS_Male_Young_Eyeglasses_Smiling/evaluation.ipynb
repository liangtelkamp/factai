{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1762 generated images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ImgDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import clip\n",
    "import torch\n",
    "import argparse\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "from cleanfid import fid\n",
    "\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = glob.glob(os.path.join(self.root_dir, '*.png'))\n",
    "        self.file_list += glob.glob(os.path.join(self.root_dir, '*.jpg'))\n",
    "        # If there is a subfolder, add it to the file list\n",
    "        subfolders = [f.path for f in os.scandir(self.root_dir) if f.is_dir()] # Liang\n",
    "        for subfolder in subfolders: # Liang\n",
    "            self.file_list += glob.glob(os.path.join(subfolder, '*.png')) # Liang\n",
    "            self.file_list += glob.glob(os.path.join(subfolder, '*.jpg')) # Liang\n",
    "        print('Found {} generated images.'.format(len(self.file_list)))\n",
    "\n",
    "        self.transforms = Compose([\n",
    "            Resize(224, interpolation=BICUBIC),\n",
    "            CenterCrop(224),\n",
    "            _convert_image_to_rgb,\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_list[idx])\n",
    "        img = self.transforms(img)\n",
    "        return img\n",
    "\n",
    "def eval(path, CLASSES, device):\n",
    "\n",
    "    eval_dataset = ImgDataset(root_dir=path)\n",
    "    eval_loader = DataLoader(\n",
    "        dataset=eval_dataset,\n",
    "        batch_size=16,\n",
    "        num_workers=8,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    clip_model.eval()\n",
    "    text = clip.tokenize(CLASSES).to(device)\n",
    "\n",
    "    img_pred_cls_list = []\n",
    "\n",
    "    for i, data in enumerate(eval_loader):\n",
    "        img = data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_per_image, _ = clip_model(img, text)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()  # (bs, n_text_query)\n",
    "\n",
    "            # select the max and set as label\n",
    "            for j in np.argmax(probs, axis=1):\n",
    "                img_pred_cls_list.append(j)\n",
    "\n",
    "    num_each_cls_list = []\n",
    "    for k in range(len(CLASSES)):\n",
    "        num_each_cls = len(np.where(np.array(img_pred_cls_list) == k)[0])\n",
    "        num_each_cls_list.append(num_each_cls)\n",
    "        print(\"{}: total pred: {} | ratio: {}\".format(CLASSES[k], num_each_cls, num_each_cls / len(eval_dataset)))\n",
    "\n",
    "    return num_each_cls_list\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    CLASSES_prompts = ['a', 'b']\n",
    "    length = len(CLASSES_prompts)\n",
    "    device_ = \"cuda\".format(args.device) if torch.cuda.is_available() else \"cpu\" # Liang\n",
    "    img_folder = '/Users/liangtelkamp/Documents/GitHub/factai/HPS_Male_Young_Eyeglasses_Smiling'\n",
    "    # evaluate\n",
    "    num_each_cls_list = eval(img_folder, CLASSES_prompts, device_)\n",
    "\n",
    "    # get the ratio\n",
    "    each_cls_ratio = num_each_cls_list/np.sum(num_each_cls_list)\n",
    "\n",
    "    # compute KL\n",
    "    uniform_distribution = np.ones(length)/length\n",
    "\n",
    "    KL1 = np.sum(scipy.special.kl_div(each_cls_ratio, uniform_distribution))\n",
    "    KL2 = scipy.stats.entropy(each_cls_ratio, uniform_distribution)\n",
    "    assert round(KL1, 4) == round(KL2, 4)\n",
    "\n",
    "    print(\"For Class {}, KL Divergence is {:4f}\".format(CLASSES_prompts, KL1))\n",
    "\n",
    "    score = fid.compute_fid(args.img_folder, dataset_name=\"FFHQ\", dataset_res=1024, dataset_split=\"trainval70k\")\n",
    "    print(\"FID Score is {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
